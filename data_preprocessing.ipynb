{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPna65qzYMBUTBui8vgCVl+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFJ92V7PVo-u",
        "outputId": "9a544be9-0772-457e-80a1-7ddd27acc37d"
      },
      "source": [
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import h5py\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import pickle\n",
        "# data generator class\n",
        "\n",
        "  #train,val,test별로 사진코드들을 모아놓은 리스트 생성\n",
        "def sorted_img_code_list(get_path):\n",
        "  sorted_list= list()  \n",
        "  with open(get_path, 'r') as f:\n",
        "    line = f.read().splitlines()\n",
        "    for filename in line:\n",
        "      sorted_list.append(filename)\n",
        "  return sorted_list \n",
        "\n",
        "\n",
        "def img_code_list(train_code_path, val_code_path, test_code_path):\n",
        "  train_list = sorted_img_code_list(train_code_path)\n",
        "  val_list = sorted_img_code_list(val_code_path)\n",
        "  test_list = sorted_img_code_list(test_code_path)\n",
        "  return train_list,val_list,test_list\n",
        "\n",
        "def create_tokenizer(seq):\n",
        "  t = Tokenizer()\n",
        "    #t.fit_on_texts([line for value in self.seq.values()])\n",
        "  ALL_text = list()\n",
        "  for elem in seq.values():\n",
        "    for line in elem:\n",
        "      ALL_text.append(line)\n",
        "  t.fit_on_texts(ALL_text)\n",
        "  t.word_index['<pad>'] = 0\n",
        "  t.index_word[0] = '<pad>'\n",
        "  return t\n",
        "\n",
        "def get_max(t,seq):\n",
        "  max_len =0\n",
        "  for elem in seq.values():\n",
        "    sequences = list()\n",
        "    for line in elem:\n",
        "      encoded_text = t.texts_to_sequences([line])[0]\n",
        "      sequences.append(encoded_text)\n",
        "  \n",
        "    max_expec = max(len(l) for l in sequences)\n",
        "    if max_len < max_expec :\n",
        "      max_len = max_expec  \n",
        "  return max_len\n",
        "\n",
        "\n",
        "\n",
        "def save_all_seq_data(t,seq,max_len,train_list,val_list,test_list,\n",
        "                      train_seq_path,val_seq_path,test_seq_path):\n",
        "  save_seq_data(t,seq,train_list,max_len,train_seq_path)\n",
        "  save_seq_data(t,seq,val_list,max_len,val_seq_path)\n",
        "  save_seq_data(t,seq,test_list,max_len,test_seq_path)\n",
        "\n",
        "\n",
        "def save_seq_data(t,seq,sorted_list,max_len,saved_seq_path): \n",
        "  vocab_size = len(t.word_index)+1    \n",
        "  with h5py.File(saved_seq_path, 'w') as f:\n",
        "    for elem in sorted_list:\n",
        "      text = t.texts_to_sequences(seq[elem])\n",
        "      text = pad_sequences(text, maxlen = max_len,padding = 'post')\n",
        "      f.create_dataset(elem, data = text)\n",
        "\n",
        "\n",
        "  #텍스트 정재하고 이를 이미지 파일 코드와 dict로 만들어줌\n",
        "def sequence_refining(token_path):\n",
        "  textList = list()\n",
        "  dic = {}      \n",
        "  with open(token_path, 'r') as f:\n",
        "    readed = f.read().splitlines()\n",
        "    filename =''\n",
        "    for line in readed:\n",
        "      filename, disc  = line.split('\\t')\n",
        "      rfilename, num = filename.split('#')\n",
        "      disc = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', disc)\n",
        "      disc = \"sq \"+disc+ \" eq\"\n",
        "      if rfilename in dic.keys():\n",
        "        dic[rfilename].append(disc)\n",
        "      else :\n",
        "        dic[rfilename] = [disc]      \n",
        "  return dic        \n",
        "\n",
        "#image에서 사전 훈련된 CNN(inception_V3)를 이용해 특징을 뽑은 뒤 h5 포맷으로 저장 \n",
        "def save_img_feature(sorted_save_path,img_data_directory,CNN_Model,img_list):\n",
        "  h5 = h5py.File(sorted_save_path,'w')  \n",
        "  indx=0    \n",
        "  for img in img_list:      \n",
        "    img_path = os.path.join(img_data_directory, img)\n",
        "    loaded_img = image.load_img(img_path, target_size = (299, 299))\n",
        "    loaded_img = image.img_to_array(loaded_img)\n",
        "    loaded_img = preprocess_input(loaded_img)\n",
        "    loaded_img = np.expand_dims(loaded_img, 0)\n",
        "    feature = CNN_Model.predict(loaded_img)\n",
        "    #shape = (8,8,2048) -> shape = (1,64,2048)\n",
        "    feature = tf.reshape(feature, (feature.shape[0],-1,feature.shape[3]))\n",
        "    #print(feature.shape)  \n",
        "    if indx % 100 == 0:\n",
        "      print('processing'+str(indx))\n",
        "        \n",
        "    h5.create_dataset(img, data=feature)\n",
        "    indx= indx+1\n",
        "  print(\"complet\")\n",
        "  h5.close()\n",
        "\n",
        "def save_all_img_feature(img_data_directory,CNN_model,\n",
        "                         train_list,val_list,test_list,\n",
        "                         train_feature_path,val_feature_path,test_feature_path):\n",
        "  save_img_feature(train_feature_path,img_data_directory,CNN_model, train_list)\n",
        "  save_img_feature(val_feature_path,img_data_directory,CNN_model, val_list)\n",
        "  save_img_feature(test_feature_path,img_data_directory,CNN_model, test_list)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  #model 생성\n",
        "  model = InceptionV3()\n",
        "  CNN_Model = Model(inputs=model.inputs, outputs = model.layers[-3].output)\n",
        "  base_directory = '/content/gdrive/My Drive/Colab Notebooks/image_captioning_with_attention'\n",
        "  #전처리된 데이터가 저장될 경로\n",
        "\n",
        "  #data_Base_Directory = '/content/flickr8k_dataset/Flicker8k_Dataset'\n",
        "  data_base_directory ='/content'\n",
        "  #데이터 셋이 있는 경로\n",
        "\n",
        "  \n",
        "  #이미지 데이터 경로\n",
        "  img_data_directory = os.path.join(data_base_directory,'flickr8k_dataset','Flicker8k_Dataset')\n",
        "\n",
        "  \n",
        "  saved_data_path = os.path.join(base_directory,'data')\n",
        "  #이미지 데이터 프로세싱 결과가 저장될 경로\n",
        "  train_feature_path = os.path.join(saved_data_path, 'train_features.hdf5')\n",
        "  val_feature_path = os.path.join(saved_data_path, 'val_features.hdf5')\n",
        "  test_feature_path = os.path.join(saved_data_path, 'test_features.hdf5')\n",
        "\n",
        "  #시퀀스 데이터 프로세싱 결과가 저장될 경로\n",
        "  train_seq_path = os.path.join(saved_data_path,'train_sequence.hdf5')\n",
        "  val_seq_path = os.path.join(saved_data_path,'val_sequence.hdf5')\n",
        "  test_seq_path = os.path.join(saved_data_path,'test_sequence.hdf5')\n",
        "  \n",
        "  \n",
        "  #분류된 이미지 데이터의 code가 담겨있는 txt파일 경로\n",
        "  text_path = os.path.join(data_base_directory,'flickr8k_text')\n",
        "  train_code_path = os.path.join(text_path,'Flickr_8k.trainImages.txt')\n",
        "  val_code_path = os.path.join(text_path,'Flickr_8k.devImages.txt')\n",
        "  test_code_path = os.path.join(text_path,'Flickr_8k.testImages.txt')\n",
        "  \n",
        "  #이미지 코드에 따라 저장되있는 이미지를 묘사하는 문자열들이 저장된 txt파일 경로\n",
        "  token_path = os.path.join(text_path,'Flickr8k.token.txt')\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  #코드 분류\n",
        "  train_list,val_list,test_list = img_code_list(train_code_path, val_code_path, test_code_path)\n",
        "  \n",
        "  #이미지 feature 저장\n",
        "  save_all_img_feature(img_data_directory,CNN_Model,\n",
        "                       train_list,val_list,test_list,\n",
        "                         train_feature_path,val_feature_path,test_feature_path)\n",
        "  \n",
        "  \n",
        "  #텍스트 정제\n",
        "  sequence = sequence_refining(token_path)\n",
        "  \n",
        "  #tokenizer 생성\n",
        "  tokenizer = create_tokenizer(sequence)\n",
        "\n",
        "  #max_len 구하기\n",
        "  max_len = get_max(tokenizer,sequence)\n",
        "  \n",
        "  #sequence data 저장\n",
        "  \n",
        "  save_all_seq_data(tokenizer,sequence,max_len,\n",
        "                    train_list,val_list,test_list,\n",
        "                    train_seq_path,val_seq_path,test_seq_path)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  req_token_path = os.path.join(saved_data_path,'token.pickle')\n",
        "  req_seq_path = os.path.join(saved_data_path,'sequence.pickle')\n",
        "  req_train_list_path = os.path.join(saved_data_path,'train_list.pickle')\n",
        "  req_val_list_path = os.path.join(saved_data_path,'val_list.pickle')\n",
        "  req_test_list_path = os.path.join(saved_data_path,'test_list.pickle')\n",
        "  with open(req_token_path, 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  with open(req_seq_path, 'wb') as handle:\n",
        "    pickle.dump(sequence, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  with open(req_train_list_path, 'wb') as handle:\n",
        "    pickle.dump(train_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  with open(req_val_list_path, 'wb') as handle:\n",
        "    pickle.dump(val_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  with open(req_test_list_path, 'wb') as handle:\n",
        "    pickle.dump(test_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  \n",
        "  \n",
        "  data_h5_paths = os.path.join(saved_data_path, 'needs.hdf5')\n",
        "  needs_h5 = h5py.File(data_h5_paths,'w')\n",
        "  needs_h5.create_dataset('token_path',data=token_path)\n",
        "  needs_h5.create_dataset('base_directory',data=base_directory)\n",
        "  needs_h5.create_dataset('train_code_path', data = train_code_path)\n",
        "  needs_h5.create_dataset('val_code_path', data = val_code_path)\n",
        "  needs_h5.create_dataset('test_code_path', data = test_code_path)\n",
        "  needs_h5.create_dataset('train_feature_path',data =train_feature_path)\n",
        "  needs_h5.create_dataset('val_feature_path', data = val_feature_path)\n",
        "  needs_h5.create_dataset('test_feature_path', data = test_feature_path)\n",
        "\n",
        "  needs_h5.create_dataset('train_seq_path', data = train_seq_path)\n",
        "  needs_h5.create_dataset('val_seq_path', data = val_seq_path)\n",
        "  needs_h5.create_dataset('test_seq_path', data = test_seq_path)\n",
        "  \n",
        "  needs_h5.create_dataset('max_len', data = max_len,dtype='int')\n",
        "  needs_h5.create_dataset('vocab_size', data = len(tokenizer.word_index)+1,dtype ='int')\n",
        "  needs_h5.create_dataset('req_token_path',data = req_token_path)\n",
        "  needs_h5.create_dataset('req_seq_path',data = req_seq_path)\n",
        "  needs_h5.create_dataset('req_train_list_path',data = req_train_list_path)\n",
        "  needs_h5.create_dataset('req_val_list_path',data = req_val_list_path)\n",
        "  needs_h5.create_dataset('req_test_list_path',data = req_test_list_path)  \n",
        "  \n",
        "  \n",
        "  print('end')\n",
        "  needs_h5.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing0\n",
            "processing100\n",
            "processing200\n",
            "processing300\n",
            "processing400\n",
            "processing500\n",
            "processing600\n",
            "processing700\n",
            "processing800\n",
            "processing900\n",
            "processing1000\n",
            "processing1100\n",
            "processing1200\n",
            "processing1300\n",
            "processing1400\n",
            "processing1500\n",
            "processing1600\n",
            "processing1700\n",
            "processing1800\n",
            "processing1900\n",
            "processing2000\n",
            "processing2100\n",
            "processing2200\n",
            "processing2300\n",
            "processing2400\n",
            "processing2500\n",
            "processing2600\n",
            "processing2700\n",
            "processing2800\n",
            "processing2900\n",
            "processing3000\n",
            "processing3100\n",
            "processing3200\n",
            "processing3300\n",
            "processing3400\n",
            "processing3500\n",
            "processing3600\n",
            "processing3700\n",
            "processing3800\n",
            "processing3900\n",
            "processing4000\n",
            "processing4100\n",
            "processing4200\n",
            "processing4300\n",
            "processing4400\n",
            "processing4500\n",
            "processing4600\n",
            "processing4700\n",
            "processing4800\n",
            "processing4900\n",
            "processing5000\n",
            "processing5100\n",
            "processing5200\n",
            "processing5300\n",
            "processing5400\n",
            "processing5500\n",
            "processing5600\n",
            "processing5700\n",
            "processing5800\n",
            "processing5900\n",
            "complet\n",
            "processing0\n",
            "processing100\n",
            "processing200\n",
            "processing300\n",
            "processing400\n",
            "processing500\n",
            "processing600\n",
            "processing700\n",
            "processing800\n",
            "processing900\n",
            "complet\n",
            "processing0\n",
            "processing100\n",
            "processing200\n",
            "processing300\n",
            "processing400\n",
            "processing500\n",
            "processing600\n",
            "processing700\n",
            "processing800\n",
            "processing900\n",
            "complet\n",
            "end\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}