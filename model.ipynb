{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSvGVrmG+erRZG9uYiG8Zw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeehyeongTea/image_captioning_with_attention/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTTU9qD-XxSR",
        "outputId": "087e2a4b-2096-4611-affc-ff977b979b04"
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Embedding, Dropout,BatchNormalization,Lambda, Add,Flatten,GRU\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras import regularizers,optimizers,losses\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from tensorflow.keras.utils import plot_model,to_categorical\n",
        "import pickle\n",
        "\n",
        "\n",
        "def get_path(base_directory):\n",
        "  saved_data_path = os.path.join(base_directory,'data')\n",
        "  data_h5_paths = os.path.join(saved_data_path, 'needs.hdf5')\n",
        "  needs = h5py.File(data_h5_paths, 'r')\n",
        "  train_dataset_list_path = needs['train_code_path'][()]\n",
        "  val_dataset_list_path = needs['val_code_path'][()]\n",
        "\n",
        "  train_feature_path = needs['train_feature_path'][()]\n",
        "  val_feature_path = needs['val_feature_path'][()]\n",
        "\n",
        "  train_seq_path= needs['train_seq_path'][()]\n",
        "  val_seq_path= needs['val_seq_path'][()]\n",
        "\n",
        "  max_len = needs['max_len'][()]\n",
        "  vocab_size= needs['vocab_size'][()]\n",
        "  \n",
        "  req_train_list_path = needs['req_train_list_path'][()]\n",
        "  req_val_list_path = needs['req_val_list_path'][()]\n",
        "  req_token_path = needs['req_token_path'][()]\n",
        "  return train_dataset_list_path, val_dataset_list_path, train_feature_path, val_feature_path, train_seq_path,val_seq_path, max_len,vocab_size, req_train_list_path, req_val_list_path, req_token_path\n",
        "\n",
        "\n",
        "def get_data(train_feature_path,train_seq_path, val_feature_path,val_seq_path):\n",
        "  return h5py.File(train_feature_path, 'r'), h5py.File(train_seq_path, 'r'), h5py.File(val_feature_path, 'r'),h5py.File(val_seq_path,'r')\n",
        "  \n",
        "\n",
        "#바다나우 어텐션 적용\n",
        "class Attention(Model):\n",
        "  def __init__(self, units, hidden):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = Dense(units)\n",
        "    self.W2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "    self.hidden = hidden\n",
        "  def call(self, values, query):\n",
        "    #value = (64, embedding_size)\n",
        "    #query = (batch_size, hidden_size)\n",
        "\n",
        "    query = tf.expand_dims(query,axis = 1)\n",
        "\n",
        "    score = self.V(tf.nn.tanh(self.W1(values)+ self.W2(query)))\n",
        "      \n",
        "    #score는 (64, 1)\n",
        "    attention_dist = tf.nn.softmax(score, axis = 1)\n",
        "    \n",
        "    context_vector = attention_dist * values\n",
        "    \n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    #shape = (hidden_size)\n",
        "    return context_vector,attention_dist\n",
        "  def build(self):\n",
        "    values = Input(shape = (64,self.hidden))\n",
        "    query = Input(shape = (self.hidden,))\n",
        "    \n",
        "    return Model(inputs=[values, query],outputs = self.call(values, query))\n",
        "\n",
        "\n",
        "class Encoder(Model):\n",
        "  def __init__(self, embedding_dim,reg,dropout_rate):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.dropout_layer = Dropout(dropout_rate)\n",
        "    self.fc = Dense(embedding_dim, activation = 'relu',kernel_regularizer = regularizers.l2(reg))\n",
        "\n",
        "  def call(self, input):\n",
        "    img_feature = self.dropout_layer(input)\n",
        "    img_feature = self.fc(img_feature)\n",
        "    #img_feature = BatchNormalization()(img_feature)\n",
        "    return img_feature\n",
        "\n",
        "  def build(self):\n",
        "    x = Input(shape=(64,2048))\n",
        "\n",
        "    return Model(inputs=x,outputs=self.call(x))\n",
        "\n",
        "class Decoder(Model):\n",
        "  \n",
        "  def __init__(self, max_len, embedding_size, units, vocab_size, dropout_rate, reg):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.embedding_layer = Embedding(vocab_size, embedding_size)\n",
        "    self.dropout_layer= Dropout(dropout_rate)\n",
        "    self.lstm = LSTM(units,return_sequences = True, return_state = True\n",
        "                     ,recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    self.attention = Attention(units,embedding_size).build()\n",
        "    self.units = units\n",
        "    self.fc = Dense(vocab_size,activation='softmax')\n",
        "  \n",
        "  def call(self, sequence,img, hidden):\n",
        "    \n",
        "    context_vector, attention_dist = self.attention([img, hidden])\n",
        "    sequence = self.embedding_layer(sequence)\n",
        "    sequence = self.dropout_layer(sequence)\n",
        "    #text_feature,hidden,last_state = self.lstm(sequence)\n",
        "    text_feature,hidden,last_state = self.lstm(sequence)\n",
        "    context_vector = tf.expand_dims(context_vector,1)\n",
        "    merge = Add()([context_vector,text_feature])\n",
        "    merge = tf.reshape(merge, (-1, merge.shape[2]))\n",
        "    output = self.fc(merge)\n",
        "    return output,hidden,attention_dist\n",
        "\n",
        "  def initial_state(self,batch_size):\n",
        "    return tf.zeros((batch_size, self.units))\n",
        "  def build(self):\n",
        "    sequence = Input(shape = (1,))\n",
        "    img = Input(shape = (64, self.units))\n",
        "    hidden = Input(shape = (self.units))\n",
        "    return Model(inputs=[sequence, img, hidden],outputs = self.call(sequence,img,hidden))\n",
        "\n",
        "\n",
        "#text[:,i],output,compile_loss,vocab_size\n",
        "def get_loss_acc(input, target,compile_loss,vocab_size):\n",
        "  #loss function에 reduction이 none이기 때문에 reduce_mean을 해주어야함\n",
        "  #패딩된 데이터를 취급하는 mask\n",
        "  mask = tf.math.logical_not(tf.math.equal(input,0))\n",
        "  loss = compile_loss(input,target)\n",
        "  mask = tf.cast(mask, dtype = loss.dtype)\n",
        "  loss *=mask\n",
        "  #loss 결과에 mask->0를 곱해줘 결과를 0으로 만들어준다.\n",
        "  one_hot = tf.one_hot(input,vocab_size)\n",
        "  correct = tf.equal(tf.argmax(one_hot,1),tf.argmax(target,1))\n",
        "  #print(tf.reduce_mean(tf.cast(correct, tf.float32)))\n",
        "  return tf.reduce_mean(loss),tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(encoder,decoder,img, text,\n",
        "               tokenizer,optimizer,compile_loss):\n",
        "  loss = 0\n",
        "  accuracy = 0\n",
        "  #img = tf.reshape\n",
        "  #hidden = decoder.initial_state(batch_size = text.shape[0])\n",
        "  hidden = tf.zeros((text.shape[0],512))\n",
        "  text_input = tf.expand_dims([tokenizer.word_index['sq']]*text.shape[0],1)\n",
        "  #(505, 64, 2048)\n",
        "  #(505, 38)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    img = encoder(img)\n",
        "    #max_len만큼\n",
        "    for i in range(1, text.shape[1]):\n",
        "      output,hidden,_ = decoder([text_input, img, hidden])\n",
        "      g_loss ,g_acc= get_loss_acc(text[:,i],output,compile_loss,vocab_size)\n",
        "      loss+=g_loss\n",
        "      accuracy+=g_acc\n",
        "      #print(output.shape)\n",
        "      #print(output)\n",
        "\n",
        "      text_input = tf.expand_dims(text[:,i],1)\n",
        "      \n",
        "  all_loss = (loss/int(text.shape[1]))\n",
        "  all_acc = (accuracy/int(text.shape[1]))\n",
        "  gradients = tape.gradient(loss,\n",
        "                            decoder.trainable_variables+encoder.trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients,decoder.trainable_variables+encoder.trainable_variables))\n",
        "  \n",
        "  return all_acc,all_loss\n",
        "\n",
        "@tf.function\n",
        "def test_step(encoder,decoder,img, text,\n",
        "               tokenizer,optimizer,compile_loss):\n",
        "  loss = 0\n",
        "  accuracy = 0\n",
        "  #img = tf.reshape\n",
        "  #hidden = decoder.initial_state(batch_size = text.shape[0])\n",
        "  hidden = tf.zeros((text.shape[0],512))\n",
        "  text_input = tf.expand_dims([tokenizer.word_index['sq']]*text.shape[0],1)\n",
        "  #(505, 64, 2048)\n",
        "  #(505, 38)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    img = encoder(img)\n",
        "    #max_len만큼\n",
        "    for i in range(1, text.shape[1]):\n",
        "      output,hidden,_ = decoder([text_input, img, hidden])\n",
        "      g_loss ,g_acc= get_loss_acc(text[:,i],output,compile_loss,vocab_size)\n",
        "      loss+=g_loss\n",
        "      accuracy+=g_acc\n",
        "\n",
        "      #print(output.shape)\n",
        "      #print(output)\n",
        "\n",
        "      text_input = tf.expand_dims(text[:,i],1)\n",
        "      \n",
        "  all_loss = (loss/int(text.shape[1]))\n",
        "  all_acc = (accuracy/int(text.shape[1]))\n",
        "\n",
        "  return all_acc,all_loss\n",
        "\n",
        "\n",
        "\n",
        "def get_feature_x_y(features,seq,elem):\n",
        "  f = features[elem][:]\n",
        "  text = seq[elem][:]\n",
        "  return f,text  \n",
        "\n",
        "\n",
        "def get_saved_data(data_list, feature, seq):\n",
        "  F=list()\n",
        "  T=list()\n",
        "  for elem in data_list:\n",
        "    f,t = get_feature_x_y(feature,seq,elem)\n",
        "    for i in range(len(t)):\n",
        "      F.append(f)\n",
        "      T.append(t[i])\n",
        "     \n",
        "  return np.array(F).squeeze(),np.array(T)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "  base_directory = '/content/gdrive/My Drive/Colab Notebooks/image_captioning_with_attention'\n",
        "  train_dataset_list_path, val_dataset_list_path, train_feature_path, val_feature_path, train_seq_path,val_seq_path, max_len,vocab_size, req_train_list_path, req_val_list_path, req_token_path = get_path(base_directory)\n",
        "\n",
        "\n",
        "  with open(req_train_list_path, 'rb') as handle:\n",
        "    train_list = pickle.load(handle)\n",
        "  with open(req_val_list_path,'rb') as handle:\n",
        "    val_list = pickle.load(handle)\n",
        "  with open(req_token_path,'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)       \n",
        "  #merge_model = Merge_model(512,1e-4,0.5,vocab_size,max_len,512)\n",
        "  \n",
        "  save_path = os.path.join(base_directory,'model6_saved_model_gru')\n",
        "\n",
        "  train_feature, train_seq, val_feature, val_seq= get_data(train_feature_path, train_seq_path,val_feature_path, val_seq_path)\n",
        "\n",
        "\n",
        "\n",
        "  encoder = Encoder(512,1e-4,0.5).build()\n",
        "  decoder = Decoder(max_len, 512, 512, vocab_size, 0.5, 1e-4).build()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  optimizer = optimizers.Adam()\n",
        "  compile_loss = losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "  #checkpoint_path = os.path.join(base_directory,'model6_saved_model')\n",
        "  #ckpt = tf.train.Checkpoint(merge_model = merge_model,\n",
        "  #                          optimizer=optimizer)\n",
        "  #ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "  #start_epoch = 0\n",
        "  #if ckpt_manager.latest_checkpoint:\n",
        "  #  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  #  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "      \n",
        "  \n",
        "  get_batch_list = list()\n",
        "  batch_size = 100\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "\n",
        "  val_loss = []\n",
        "  val_acc = []\n",
        "  for epoch in range(0,30):\n",
        "    get_batch_list = list()\n",
        "    batch_train_loss=[]\n",
        "    batch_train_acc=[]\n",
        "\n",
        "    for i in range(0, len(train_list)):\n",
        "      get_batch_list.append(train_list[i])\n",
        "      if i % batch_size == 0 and i != 0 :\n",
        "        img, text = get_saved_data(get_batch_list, train_feature, train_seq)\n",
        "        t_acc,t_loss = train_step(encoder,decoder,img,text,\n",
        "                                       tokenizer,optimizer,compile_loss)\n",
        "        batch_train_loss.append(t_loss)\n",
        "        batch_train_acc.append(t_acc)\n",
        "        get_batch_list.clear()\n",
        "\n",
        "    if len(get_batch_list) != 0 :\n",
        "      img, text = get_saved_data(get_batch_list, train_feature, train_seq)\n",
        "      t_acc,t_loss = train_step(encoder,decoder,img,text,\n",
        "                                       tokenizer,optimizer,compile_loss)\n",
        "      batch_train_loss.append(t_loss)\n",
        "      batch_train_acc.append(t_acc)\n",
        "      get_batch_list.clear()\n",
        "    train_loss.append(np.mean(batch_train_loss))\n",
        "    train_acc.append(np.mean(batch_train_acc))\n",
        "\n",
        "    batch_val_loss=[]\n",
        "    batch_val_acc = []\n",
        "    for i in range(0, len(val_list)):\n",
        "      get_batch_list.append(val_list[i])\n",
        "      if i % batch_size == 0 and i != 0 :\n",
        "        img, text = get_saved_data(get_batch_list, val_feature, val_seq)\n",
        "        v_acc,v_loss = test_step(encoder,decoder,img,text,\n",
        "                                       tokenizer,optimizer,compile_loss)\n",
        "        batch_val_loss.append(v_loss)\n",
        "        batch_val_acc.append(v_acc)\n",
        "        get_batch_list.clear()\n",
        "\n",
        "    if len(get_batch_list) != 0 :\n",
        "      img, text = get_saved_data(get_batch_list, val_feature, val_seq)\n",
        "      v_acc,v_loss = test_step(encoder,decoder,img,text,\n",
        "                                       tokenizer,optimizer,compile_loss)\n",
        "      batch_val_loss.append(v_loss)\n",
        "      batch_val_acc.append(v_acc)\n",
        "\n",
        "    val_loss.append(np.mean(batch_val_loss))\n",
        "    val_acc.append(np.mean(batch_val_acc))\n",
        "    print('epoch {0:4d} train acc {1:0.3f} loss {2:0.3f} val acc {3:0.3f} loss {4:0.3f}'.\n",
        "          format(epoch, np.mean(train_acc), np.mean(train_loss), np.mean(val_acc), np.mean(val_loss)))\n",
        "    encoder_path = os.path.join(save_path,'encoder_model_{0:02d}_vacc_{1:0.3f}_vloss_{2:0.3f}_acc{3:0.3f}_loss{4:0.3f}.h5'.\n",
        "                        format(epoch, np.mean(val_acc), np.mean(val_loss), np.mean(train_acc), np.mean(train_loss)))\n",
        "    decoder_path = os.path.join(save_path,'decoder_model_{0:02d}_vacc_{1:0.3f}_vloss_{2:0.3f}_acc{3:0.3f}_loss{4:0.3f}.h5'.\n",
        "                        format(epoch, np.mean(val_acc), np.mean(val_loss), np.mean(train_acc), np.mean(train_loss)))\n",
        "    \n",
        "    #ckpt_manager.save()\n",
        "    encoder.save(encoder_path)\n",
        "    decoder.save(decoder_path)\n",
        "\n",
        "\n",
        "  train_feature.close()\n",
        "  train_seq.close()\n",
        "  \n",
        "  val_feature.close()\n",
        "  val_seq.close()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch    0 train acc 0.049 loss 1.700 val acc 0.066 loss 1.517\n",
            "epoch    1 train acc 0.063 loss 1.530 val acc 0.077 loss 1.403\n",
            "epoch    2 train acc 0.074 loss 1.407 val acc 0.085 loss 1.329\n",
            "epoch    3 train acc 0.082 loss 1.317 val acc 0.090 loss 1.281\n",
            "epoch    4 train acc 0.088 loss 1.250 val acc 0.093 loss 1.249\n",
            "epoch    5 train acc 0.092 loss 1.197 val acc 0.096 loss 1.226\n",
            "epoch    6 train acc 0.096 loss 1.153 val acc 0.097 loss 1.210\n",
            "epoch    7 train acc 0.099 loss 1.115 val acc 0.098 loss 1.199\n",
            "epoch    8 train acc 0.101 loss 1.083 val acc 0.099 loss 1.191\n",
            "epoch    9 train acc 0.104 loss 1.054 val acc 0.100 loss 1.184\n",
            "epoch   10 train acc 0.106 loss 1.027 val acc 0.100 loss 1.180\n",
            "epoch   11 train acc 0.108 loss 1.004 val acc 0.101 loss 1.177\n",
            "epoch   12 train acc 0.110 loss 0.983 val acc 0.101 loss 1.175\n",
            "epoch   13 train acc 0.112 loss 0.964 val acc 0.102 loss 1.173\n",
            "epoch   14 train acc 0.114 loss 0.946 val acc 0.102 loss 1.174\n",
            "epoch   15 train acc 0.116 loss 0.930 val acc 0.102 loss 1.176\n",
            "epoch   16 train acc 0.117 loss 0.914 val acc 0.102 loss 1.177\n",
            "epoch   17 train acc 0.119 loss 0.900 val acc 0.102 loss 1.179\n",
            "epoch   18 train acc 0.120 loss 0.886 val acc 0.102 loss 1.181\n",
            "epoch   19 train acc 0.122 loss 0.873 val acc 0.102 loss 1.183\n",
            "epoch   20 train acc 0.123 loss 0.862 val acc 0.102 loss 1.186\n",
            "epoch   21 train acc 0.124 loss 0.850 val acc 0.103 loss 1.190\n",
            "epoch   22 train acc 0.126 loss 0.839 val acc 0.103 loss 1.194\n",
            "epoch   23 train acc 0.127 loss 0.829 val acc 0.102 loss 1.198\n",
            "epoch   24 train acc 0.128 loss 0.819 val acc 0.102 loss 1.202\n",
            "epoch   25 train acc 0.129 loss 0.810 val acc 0.102 loss 1.207\n",
            "epoch   26 train acc 0.130 loss 0.801 val acc 0.102 loss 1.210\n",
            "epoch   27 train acc 0.131 loss 0.793 val acc 0.102 loss 1.212\n",
            "epoch   28 train acc 0.132 loss 0.785 val acc 0.102 loss 1.215\n",
            "epoch   29 train acc 0.133 loss 0.777 val acc 0.102 loss 1.218\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}