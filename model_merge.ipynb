{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_merge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORcgZ1ZdNW39310y7DRXwO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeehyeongTea/image_captioning_with_attention/blob/main/model_merge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_MA-6uqUfTO",
        "outputId": "fde31741-bab2-4a95-86f2-0a4cf8ee3684"
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Embedding, Dropout,BatchNormalization,Lambda, Add,Flatten,GRU\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras import regularizers,optimizers,losses\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from tensorflow.keras.utils import plot_model,to_categorical\n",
        "import pickle\n",
        "\n",
        "\n",
        "def get_path(base_directory):\n",
        "  saved_data_path = os.path.join(base_directory,'data')\n",
        "  data_h5_paths = os.path.join(saved_data_path, 'needs.hdf5')\n",
        "  needs = h5py.File(data_h5_paths, 'r')\n",
        "  train_dataset_list_path = needs['train_code_path'][()]\n",
        "  val_dataset_list_path = needs['val_code_path'][()]\n",
        "\n",
        "  train_feature_path = needs['train_feature_path'][()]\n",
        "  val_feature_path = needs['val_feature_path'][()]\n",
        "\n",
        "  train_seq_path= needs['train_seq_path'][()]\n",
        "  val_seq_path= needs['val_seq_path'][()]\n",
        "\n",
        "  max_len = needs['max_len'][()]\n",
        "  vocab_size= needs['vocab_size'][()]\n",
        "  \n",
        "  req_train_list_path = needs['req_train_list_path'][()]\n",
        "  req_val_list_path = needs['req_val_list_path'][()]\n",
        "  req_token_path = needs['req_token_path'][()]\n",
        "  return train_dataset_list_path, val_dataset_list_path, train_feature_path, val_feature_path, train_seq_path,val_seq_path, max_len,vocab_size, req_train_list_path, req_val_list_path, req_token_path\n",
        "\n",
        "\n",
        "def get_data(train_feature_path,train_seq_path, val_feature_path,val_seq_path):\n",
        "  return h5py.File(train_feature_path, 'r'), h5py.File(train_seq_path, 'r'), h5py.File(val_feature_path, 'r'),h5py.File(val_seq_path,'r')\n",
        "\"\"\"  \n",
        "Q = Query : t-1 시점의 디코더 셀에서의 은닉 상태 -> hidden\n",
        "K = Keys : 모든 시점의 인코더 셀의 은닉 상태들 => \n",
        "V = Values : 모든 시점의 인코더 셀의 은닉 상태들 => img feature\n",
        "\"\"\"\n",
        "#바다나우 어텐션 적용\n",
        "class Attention(Model):\n",
        "  def __init__(self, units, embedding_size):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = Dense(units)\n",
        "    self.W2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "    self.embedding_size = embedding_size\n",
        "    self.units = units\n",
        "  def call(self, values, query):\n",
        "    #value = (64, embedding_size)\n",
        "    #query = (batch_size, hidden_size)\n",
        "\n",
        "    query = tf.expand_dims(query,axis = 1)\n",
        "\n",
        "    score = self.V(\n",
        "        tf.nn.tanh(self.W1(values)+ self.W2(query)))\n",
        "      \n",
        "\n",
        "    attention_dist = tf.nn.softmax(score, axis = 1)\n",
        "    context_vector = attention_dist * values\n",
        "    \n",
        "    \n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "\n",
        "    return context_vector,attention_dist\n",
        "  def build(self):\n",
        "    values = Input(shape = (64,self.embedding_size))\n",
        "    query = Input(shape = (self.units,))\n",
        "    \n",
        "    return Model(inputs=[values, query],outputs = self.call(values, query))\n",
        "\n",
        "class Encoder(Model):\n",
        "  def __init__(self, embedding_dim,reg):\n",
        "    super(Encoder,self).__init__()\n",
        "\n",
        "    self.fc = Dense(embedding_dim, activation = 'relu',kernel_regularizer = regularizers.l2(reg))\n",
        "\n",
        "  def call(self, input):\n",
        "\n",
        "    img_feature = self.fc(input)\n",
        "\n",
        "    return img_feature\n",
        "\n",
        "  def build(self):\n",
        "    x = Input(shape=(64,2048))\n",
        "\n",
        "    return Model(inputs=x,outputs=self.call(x))\n",
        "\n",
        "class Decoder(Model):\n",
        "  \n",
        "  def __init__(self, max_len, embedding_size, units, vocab_size, reg):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.units = units\n",
        "    self.embedding_size = embedding_size\n",
        "    self.embedding_layer = Embedding(vocab_size, embedding_size)\n",
        "    self.lstm = LSTM(units)\n",
        "\n",
        "    self.attention = Attention(units,embedding_size).build()\n",
        "    self.fc1 = Dense(self.units, activation = 'relu', kernel_regularizer = regularizers.l2(reg))\n",
        "    self.fc2 = Dense(vocab_size, activation = 'softmax', kernel_regularizer = regularizers.l2(reg))\n",
        "  \n",
        "  def call(self, sequence,img, hidden):\n",
        "    \n",
        "    context_vector, attention_dist = self.attention([img, hidden])\n",
        "    sequence = self.embedding_layer(sequence)\n",
        "\n",
        "    output = self.lstm(sequence)\n",
        "\n",
        "    x = self.fc1(context_vector)\n",
        "\n",
        "    merge = Add()([output, x])\n",
        "    merge = self.fc2(merge)\n",
        "    \n",
        "    return merge,output,attention_dist\n",
        "\n",
        "  def build(self):\n",
        "    sequence = Input(shape = (1,))\n",
        "    img = Input(shape = (64, self.embedding_size))\n",
        "    hidden = Input(shape = (self.units))\n",
        "    return Model(inputs=[sequence, img, hidden],outputs = self.call(sequence,img,hidden))\n",
        "\n",
        "\n",
        "#text[:,i],output,compile_loss,vocab_size\n",
        "def get_loss_acc(input, target,compile_loss,vocab_size):\n",
        "  #loss function에 reduction이 none이기 때문에 reduce_mean을 해주어야함\n",
        "  #패딩된 데이터를 취급하는 mask\n",
        "  mask = tf.math.logical_not(tf.math.equal(input,0))\n",
        "  loss = compile_loss(input,target)\n",
        "  mask = tf.cast(mask, dtype = loss.dtype)\n",
        "  loss *=mask\n",
        "  #loss 결과에 mask->0를 곱해줘 결과를 0으로 만들어준다.\n",
        "  one_hot = tf.one_hot(input,vocab_size)\n",
        "  correct = tf.equal(tf.argmax(one_hot,1),tf.argmax(target,1))\n",
        "\n",
        "  return tf.reduce_mean(loss),tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(encoder,decoder,img, text,\n",
        "               tokenizer,optimizer,compile_loss):\n",
        "  loss = 0\n",
        "  accuracy = 0\n",
        "\n",
        "  hidden = tf.zeros((text.shape[0],512))\n",
        "  text_input = tf.expand_dims([tokenizer.word_index['sq']]*text.shape[0],1)\n",
        "\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    img = encoder(img)\n",
        "    #max_len만큼\n",
        "    for i in range(1, text.shape[1]):\n",
        "      output,hidden,_ = decoder([text_input, img, hidden])\n",
        "      g_loss ,g_acc= get_loss_acc(text[:,i],output,compile_loss,vocab_size)\n",
        "      loss+=g_loss\n",
        "      accuracy+=g_acc\n",
        "\n",
        "\n",
        "      text_input = tf.expand_dims(text[:,i],1)\n",
        "      \n",
        "  all_loss = (loss/int(text.shape[1]))\n",
        "  all_acc = (accuracy/int(text.shape[1]))\n",
        "  gradients = tape.gradient(loss,\n",
        "                            decoder.trainable_variables+encoder.trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients,decoder.trainable_variables+encoder.trainable_variables))\n",
        "  \n",
        "  return all_acc,all_loss\n",
        "\n",
        "@tf.function\n",
        "def test_step(encoder,decoder,img, text,\n",
        "               tokenizer,optimizer,compile_loss):\n",
        "  loss = 0\n",
        "  accuracy = 0\n",
        "\n",
        "  hidden = tf.zeros((text.shape[0],512))\n",
        "  text_input = tf.expand_dims([tokenizer.word_index['sq']]*text.shape[0],1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    img = encoder(img)\n",
        "    #max_len만큼\n",
        "    for i in range(1, text.shape[1]):\n",
        "      output,hidden,_ = decoder([text_input, img, hidden])\n",
        "      g_loss ,g_acc= get_loss_acc(text[:,i],output,compile_loss,vocab_size)\n",
        "      loss+=g_loss\n",
        "      accuracy+=g_acc\n",
        "\n",
        "\n",
        "      text_input = tf.expand_dims(text[:,i],1)\n",
        "      \n",
        "  all_loss = (loss/int(text.shape[1]))\n",
        "  all_acc = (accuracy/int(text.shape[1]))\n",
        "\n",
        "  return all_acc,all_loss\n",
        "\n",
        "\n",
        "\n",
        "def get_feature_x_y(features,seq,elem):\n",
        "  f = features[elem][:]\n",
        "  text = seq[elem][:]\n",
        "  return f,text  \n",
        "\n",
        "\n",
        "def get_saved_data(data_list, feature, seq):\n",
        "  F=list()\n",
        "  T=list()\n",
        "  for elem in data_list:\n",
        "    f,t = get_feature_x_y(feature,seq,elem)\n",
        "    for i in range(len(t)):\n",
        "      F.append(f)\n",
        "      T.append(t[i])\n",
        "     \n",
        "  return np.array(F).squeeze(),np.array(T)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "  base_directory = '/content/gdrive/My Drive/Colab Notebooks/image_captioning_with_attention'\n",
        "  train_dataset_list_path, val_dataset_list_path, train_feature_path, val_feature_path, train_seq_path,val_seq_path, max_len,vocab_size, req_train_list_path, req_val_list_path, req_token_path = get_path(base_directory)\n",
        "\n",
        "\n",
        "  with open(req_train_list_path, 'rb') as handle:\n",
        "    train_list = pickle.load(handle)\n",
        "  with open(req_val_list_path,'rb') as handle:\n",
        "    val_list = pickle.load(handle)\n",
        "  with open(req_token_path,'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)       \n",
        "  \n",
        "  save_path = os.path.join(base_directory,'saved_model_merge')\n",
        "\n",
        "  train_feature, train_seq, val_feature, val_seq= get_data(train_feature_path, train_seq_path,val_feature_path, val_seq_path)\n",
        "\n",
        "\n",
        "  embedding_size = 256\n",
        "  units = 512\n",
        "  reg = 1e-4\n",
        "  \n",
        "\n",
        "  encoder = Encoder(embedding_size,reg).build()\n",
        "  decoder = Decoder(max_len, embedding_size, units, vocab_size,reg).build()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  optimizer = optimizers.Adam()\n",
        "  compile_loss = losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "  \n",
        "  get_batch_list = list()\n",
        "  batch_size = 100\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "\n",
        "  val_loss = []\n",
        "  val_acc = []\n",
        "  for epoch in range(0,20):\n",
        "    get_batch_list = list()\n",
        "    batch_train_loss=[]\n",
        "    batch_train_acc=[]\n",
        "\n",
        "    for i in range(0, len(train_list)):\n",
        "      get_batch_list.append(train_list[i])\n",
        "      if i % batch_size == 0 and i != 0 :\n",
        "        img, text = get_saved_data(get_batch_list, train_feature, train_seq)\n",
        "        t_acc,t_loss = train_step(encoder,decoder,img,text,\n",
        "                                       tokenizer,optimizer,compile_loss)\n",
        "        batch_train_loss.append(t_loss)\n",
        "        batch_train_acc.append(t_acc)\n",
        "        get_batch_list.clear()\n",
        "\n",
        "    if len(get_batch_list) != 0 :\n",
        "      img, text = get_saved_data(get_batch_list, train_feature, train_seq)\n",
        "      t_acc,t_loss = train_step(encoder,decoder,img,text,\n",
        "                                       tokenizer,optimizer,compile_loss)\n",
        "      batch_train_loss.append(t_loss)\n",
        "      batch_train_acc.append(t_acc)\n",
        "      get_batch_list.clear()\n",
        "    train_loss.append(np.mean(batch_train_loss))\n",
        "    train_acc.append(np.mean(batch_train_acc))\n",
        "\n",
        "    batch_val_loss=[]\n",
        "    batch_val_acc = []\n",
        "    for i in range(0, len(val_list)):\n",
        "      get_batch_list.append(val_list[i])\n",
        "      if i % batch_size == 0 and i != 0 :\n",
        "        img, text = get_saved_data(get_batch_list, val_feature, val_seq)\n",
        "        v_acc,v_loss = test_step(encoder,decoder,img,text,\n",
        "                                       tokenizer,optimizer,compile_loss)\n",
        "        batch_val_loss.append(v_loss)\n",
        "        batch_val_acc.append(v_acc)\n",
        "        get_batch_list.clear()\n",
        "\n",
        "    if len(get_batch_list) != 0 :\n",
        "      img, text = get_saved_data(get_batch_list, val_feature, val_seq)\n",
        "      v_acc,v_loss = test_step(encoder,decoder,img,text,\n",
        "                                       tokenizer,optimizer,compile_loss)\n",
        "      batch_val_loss.append(v_loss)\n",
        "      batch_val_acc.append(v_acc)\n",
        "\n",
        "    val_loss.append(np.mean(batch_val_loss))\n",
        "    val_acc.append(np.mean(batch_val_acc))\n",
        "    print('epoch {0:4d} train acc {1:0.3f} loss {2:0.3f} val acc {3:0.3f} loss {4:0.3f}'.\n",
        "          format(epoch, np.mean(train_acc), np.mean(train_loss), np.mean(val_acc), np.mean(val_loss)))\n",
        "    encoder_path = os.path.join(save_path,'encoder_model_{0:02d}_vacc_{1:0.3f}_vloss_{2:0.3f}_acc{3:0.3f}_loss{4:0.3f}.h5'.\n",
        "                        format(epoch, np.mean(val_acc), np.mean(val_loss), np.mean(train_acc), np.mean(train_loss)))\n",
        "    decoder_path = os.path.join(save_path,'decoder_model_{0:02d}_vacc_{1:0.3f}_vloss_{2:0.3f}_acc{3:0.3f}_loss{4:0.3f}.h5'.\n",
        "                        format(epoch, np.mean(val_acc), np.mean(val_loss), np.mean(train_acc), np.mean(train_loss)))\n",
        "    \n",
        "\n",
        "    encoder.save(encoder_path)\n",
        "    decoder.save(decoder_path)\n",
        "\n",
        "\n",
        "  train_feature.close()\n",
        "  train_seq.close()\n",
        "  \n",
        "  val_feature.close()\n",
        "  val_seq.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch    0 train acc 0.049 loss 1.707 val acc 0.066 loss 1.527\n",
            "epoch    1 train acc 0.061 loss 1.548 val acc 0.072 loss 1.434\n",
            "epoch    2 train acc 0.070 loss 1.440 val acc 0.080 loss 1.365\n",
            "epoch    3 train acc 0.078 loss 1.357 val acc 0.086 loss 1.316\n",
            "epoch    4 train acc 0.083 loss 1.292 val acc 0.089 loss 1.282\n",
            "epoch    5 train acc 0.088 loss 1.241 val acc 0.092 loss 1.257\n",
            "epoch    6 train acc 0.091 loss 1.199 val acc 0.094 loss 1.238\n",
            "epoch    7 train acc 0.094 loss 1.163 val acc 0.096 loss 1.225\n",
            "epoch    8 train acc 0.097 loss 1.133 val acc 0.097 loss 1.214\n",
            "epoch    9 train acc 0.099 loss 1.106 val acc 0.098 loss 1.206\n",
            "epoch   10 train acc 0.101 loss 1.081 val acc 0.099 loss 1.199\n",
            "epoch   11 train acc 0.103 loss 1.059 val acc 0.099 loss 1.194\n",
            "epoch   12 train acc 0.104 loss 1.039 val acc 0.100 loss 1.190\n",
            "epoch   13 train acc 0.106 loss 1.021 val acc 0.101 loss 1.187\n",
            "epoch   14 train acc 0.108 loss 1.004 val acc 0.101 loss 1.184\n",
            "epoch   15 train acc 0.109 loss 0.989 val acc 0.102 loss 1.183\n",
            "epoch   16 train acc 0.110 loss 0.974 val acc 0.102 loss 1.181\n",
            "epoch   17 train acc 0.112 loss 0.961 val acc 0.102 loss 1.180\n",
            "epoch   18 train acc 0.113 loss 0.948 val acc 0.102 loss 1.179\n",
            "epoch   19 train acc 0.114 loss 0.936 val acc 0.103 loss 1.179\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}